{
  
    
        "post0": {
            "title": "Scraping Netflix Subtitle Data: A Small Example",
            "content": "Getting at Netflix Subtitles and Exploring a Dave Chappelle joke setup . . This is a little example of parsing Netflix subtitle data. Here we look at Dave Chappelle&#39;s comedy special Equanimity and the story that he uses to setup a punchline he alludes to at the start of the show. . Note About Getting Subtitles From Netflix: I looked at this GitHub repo for a little, which helped me find the subtitle file, but parsing it wasn&#39;t working. So, I saved it off as an XML file and did it myself with python. . Find episode to get subtitles | Developer Tools &gt; Network &gt; file starting with ?o= | Save off as an .xml file | from bs4 import BeautifulSoup import pandas as pd pd.set_option(&#39;display.max_colwidth&#39;, 300) pd.set_option(&#39;display.max_rows&#39;, 300) from collections import Counter import re . . def make_netflix_subtitle_df(netflix_xml_file): &#39;&#39;&#39; Formatting from XML file of Netflix subtitles into a dataframe &#39;&#39;&#39; # open file with open(netflix_xml_file, &#39;r&#39;) as f: contents = f.read() # replace &lt;br&gt; tags with space, important to get a full string per timestamp contents = contents.replace(&#39;&lt;br/&gt;&#39;, &#39; &#39;) # beautiful soup soup = BeautifulSoup(contents, &#39;html.parser&#39;) # grab p tags, loop thru and get tags/text from each subtitles = soup.find_all(&#39;p&#39;) df_dict = {} for ind, timestamp in enumerate(subtitles): # print(timestamp.attrs) # print(timestamp.text) df_dict[ind] = timestamp.attrs df_dict[ind][&#39;text&#39;] = timestamp.text # dataframe df = pd.DataFrame(df_dict.values())[[&#39;begin&#39;, &#39;end&#39;, &#39;text&#39;]] # format df[&#39;begin&#39;] = pd.to_numeric(df.begin.str.replace(&#39;t&#39;, &#39;&#39;)) df[&#39;end&#39;] = pd.to_numeric(df.end.str.replace(&#39;t&#39;, &#39;&#39;)) return df . Dave Chappelle Fishbowl Story . How many words before he wraps back around to the original punchline? | How long is that gap? | . df = make_netflix_subtitle_df(&#39;chappelle-equanimity.xml&#39;) . df.head() . begin end text . 0 63813750 | 72572500 | [&quot;Killing Me Softly with His Song&quot; playing] | . 1 73406667 | 100934167 | [woman vocalizes] | . 2 111778334 | 149732917 | ♪ I heard he sang a good song ♪ | . 3 150567084 | 191024167 | ♪ I heard he had a style ♪ -[camera shutter clicks] | . 4 191858334 | 241074167 | ♪ And so I came to see him To listen for a while ♪ | . interim_story = df.loc[57:146] . interim_story.head() . begin end text . 57 2051215834 | 2064145417 | You know what&#39;s weird? | . 58 2072487084 | 2090421667 | I&#39;ve always been this talented. | . 59 2105853750 | 2123788334 | I can&#39;t remember a time when I wasn&#39;t. | . 60 2124622500 | 2159240417 | You know, when I was growing up, I was probably about eight years old, | . 61 2160074584 | 2188436250 | and at the time,  we were living in Silver Spring. | . How Many Words? . story_text = interim_story.text.str.cat(sep = &#39; &#39;) . text = story_text text = text.lower() text = re.sub(&quot;[^ w ]&quot;, &quot;&quot;, text) text . &#39;you know whats weird ive always been this talented i cant remember a time when i wasnt you know when i was growing up i was probably about eight years old and at the time we were living in silver spring yeah yes common misconception about me and dc a lot of people think im from the hood thats not true but i never bothered to correct anybody because i wanted the streets to embrace me as a matter of fact i kept it up as a ruse like sometimes ill hang out with rappers like nas and them and these motherfuckers start talking about the projects yo it was wild in the pjs yo and ill be like word nigga word but i dont know i have no idea my parents did just well enough so that i could grow up poor around white people to be honest whennas and them talk about the projects nigga i used to get jealous because it sounded fun everybody in the projects was poor and thats fair but if you were poor in silver spring nigga it felt like it was only happening to you nas does not know the pain of that first sleepover at a white friends house when you come back home on sunday and just look at your parents like yall need to step your game up everything at timmys house works remember the first time you saw that the cold winter and to be at a white friends house and see them motherfuckers in their living room without their coats on timmy was one of my first white friends like in my life man good dude too he moved to silver spring from utah of all places i guess his family was affiliated with that mormon church they got down there me and him used to hang out one day i was at his house just hanging out and timmy says dave why dont you stay for dinner tonight i said oh man id love to but i cant if im not home before dark my mother will kill me that was a lie my mother had several jobs i hadnt seen her in three or four days and the only reason i lied to timmy was because at that point in my life it was my experience that white dinner wasnt delicious id rather go home and fry some bologna or some shit like that but then old timmy threw me a curveball i wasnt expecting he said oh its too bad you cant stay dave cause mama made stove top stuffing i said what the fuck stove top hold on nigga let me make some phone calls real quick i had seen that commercial so many times i had dreamt of getting my hands on some of that stove top stuffing finally i met a motherfucker that actually had a box of stove top in the house i couldnt miss this opportunity so i pretended to call my mother then i came back and i said timmy timmy youre not gonna believe this great news mom said i can stay he said fantastic he said why dont you come with me and well help set the table and then we can say the blessing i had no interest in setting this motherfuckers table or saying these crazyass mormon prayers i just wanted the goddamn stuffing i told timmy you know what id love to help but let me go wash my hands first my plan was simple wash my hands slowly and by the time im done the table will be set the blessing will be said and all that there will be left to do is eat went to the bathroom i wash my hands very slowly i must have been in there for about ten minutes and suddenly one of his mothers came to the door she was like hi david right i said yes maam she said timmy tells me that youre planning on staying for dinner i said i hope thats not a problem maam she says no its no problem wed love to have you its just that we werent expecting company and im afraid theres not enoughstove top stuffing for everybody&#39; . . words = text.split(&#39; &#39;) word_count = Counter(words) . len(list(word_count.elements())) . 712 . How Long of a Gap? . interim_story.head() . begin end text . 57 2051215834 | 2064145417 | You know what&#39;s weird? | . 58 2072487084 | 2090421667 | I&#39;ve always been this talented. | . 59 2105853750 | 2123788334 | I can&#39;t remember a time when I wasn&#39;t. | . 60 2124622500 | 2159240417 | You know, when I was growing up, I was probably about eight years old, | . 61 2160074584 | 2188436250 | and at the time,  we were living in Silver Spring. | . start = interim_story.iat[0, 0] end = interim_story.iat[-1, 1] . duration = end - start . The duration is provided as 10^7 seconds. . minutes_full = duration/(10 ** 7)/60 minutes_full . 5.243432638333333 . minutes = int(minutes_full) seconds_frac = minutes_full % 1 seconds = int(seconds_frac * 60) . print(f&quot;Story lasts for {minutes} minutes, {seconds} seconds&quot;) . Story lasts for 5 minutes, 14 seconds . Conclusion . Example of parsing Netflix subtitles. Data includes timestamp for a given subtitle and the associated text. | . What&#39;s Useful for Later: . Function for parsing Netflix .xml subtitle files | . Going Further . I could see attempting to recreate work I have done previously on the use of swear words in comedy specials | . Till next time! . . Image Credit . comedy mask by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/python/pandas/beautifulsoup/2020/10/29/parse-netflix-subtitles.html",
            "relUrl": "/python/pandas/beautifulsoup/2020/10/29/parse-netflix-subtitles.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "I love everything() from the tidyverse",
            "content": ". It is probably true that I love everything from the tidyverse, full stop, but in this case parens matter. Reordering columns happens a bunch and it can be a pain, but everything() allows for easy manipulation of columns in a dataframe. Let&#39;s give it a go. . How I&#39;d do it in Python . Say I have some data on rainfall and I want to add a column for the decade in which the reading was taken (Source: tidytuesday). . install.packages(&#39;reticulate&#39;) . also installing the dependency ‘rappdirs’ Updating HTML index of packages in &#39;.Library&#39; Making &#39;packages.html&#39; ... done . library(reticulate) . pd = import(&quot;pandas&quot;) . df = pd$read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-07/rainfall.csv&#39;) df[&quot;decade&quot;] = df$year %/% 10 * 10 . head(df) . A data.frame: 6 × 12 station_codecity_nameyearmonthdayrainfallperiodqualitylatlongstation_namedecade . &lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;list&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt; . 19151 | Perth | 1967 | 1 | 1 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . 29151 | Perth | 1967 | 1 | 2 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . 39151 | Perth | 1967 | 1 | 3 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . 49151 | Perth | 1967 | 1 | 4 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . 59151 | Perth | 1967 | 1 | 5 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . 69151 | Perth | 1967 | 1 | 6 | NaN | NaN | NaN | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | 1960 | . If I create a new column, it is tacked onto the end of the dataframe, but I&#39;d prefer the decade and year columns to be closer together. In python, I find this to be kind of a pain since you need to know the column index and you have to do conversions and junk (insert is also an option, but you have to drop duplicate columns...it&#39;s a whole deal): . cols = list(df.columns) # manual ordering of list...ewww cols = cols[:3] + [cols[-1]] + cols[3:-1] df[cols].head() . How Tidyverse does it . I find it much easier to manipulate the column names. In comes everything() to the rescue. Combining with select(), you can get pretty fancy. . library(tidyverse) rainfall &lt;- suppressMessages(read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-07/rainfall.csv&#39;)) %&gt;% mutate(decade = year %/% 10 * 10) . rainfall %&gt;% select(station_code:year, decade, everything()) %&gt;% head() . A tibble: 6 × 12 station_codecity_nameyeardecademonthdayrainfallperiodqualitylatlongstation_name . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt; . 009151 | Perth | 1967 | 1960 | 01 | 01 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . 009151 | Perth | 1967 | 1960 | 01 | 02 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . 009151 | Perth | 1967 | 1960 | 01 | 03 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . 009151 | Perth | 1967 | 1960 | 01 | 04 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . 009151 | Perth | 1967 | 1960 | 01 | 05 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . 009151 | Perth | 1967 | 1960 | 01 | 06 | NA | NA | NA | -31.96 | 115.79 | Subiaco Wastewater Treatment Plant | . The reason I love everything() is you don&#39;t have to think about it. Get the thing in place then shove the rest on and it&#39;ll deal with it nicely. Also learned after writing this that last_col() is also available to help out. . Image Credit . Heart by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/r/python/pandas/tidyverse/2020/08/20/everything-from-tidyverse.html",
            "relUrl": "/r/python/pandas/tidyverse/2020/08/20/everything-from-tidyverse.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Learning R: Plotting Data in Polar Coordinates",
            "content": ". I&#39;m going through R4DS and I liked the ease with which one can map a bar chart in polar coordinates, so thought I&#39;d give it a shot with some temperature data. . Plotting in Polar Coordinates . install.packages(&quot;rattle.data&quot;) . Updating HTML index of packages in &#39;.Library&#39; Making &#39;packages.html&#39; ... done . library(tidyverse) theme_set(theme_light()) . library(rattle.data) aus_temp &lt;- weather %&gt;% select(Date, Temp9am, Temp3pm) %&gt;% pivot_longer(cols = Temp9am:Temp3pm, names_to=&quot;temp&quot;, values_to=&quot;value&quot;) %&gt;% ggplot(aes(x=Date)) + geom_line(aes(y=value, color=temp)) + labs(x=&quot;&quot;,y=&quot;Temperature (Celsius)&quot;) aus_temp + coord_polar() . Less Fun, Easier to Parse . aus_temp + coord_cartesian() . Overall . I&#39;m a big fan of dplyr and the ability to pipe things together | Polar Coordinates are cool, but can be more confusing than helpful | . Image Credit . Target by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/r/tidyverse/ggplot/2020/08/11/australia-temparature.html",
            "relUrl": "/r/tidyverse/ggplot/2020/08/11/australia-temparature.html",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A Python Blog Post I Often Reach For",
            "content": ". I find I have to do a lot of grouping in pandas and I reach for this blog post by Shane Lynn all the time to remind me how to get it done. I&#39;ve found grouping things in pandas difficult sometimes, usually when I want to create a column in the original dataframe by grouping stuff. This technique solves that and I&#39;m super grateful to it. . So Group, Already . Learning works best when you try it out yourself, so let&#39;s give it a go! . Say I have some census data and I want to group it together to get the sum of each group. . Dataset is from US Census Demographics Data on Kaggle | . import pandas as pd . df = pd.read_csv(&quot;/Users/bogart/Downloads/7001_312628_bundle_archive/acs2017_county_data.csv&quot;) df = df[[&#39;State&#39;, &#39;County&#39;, &#39;TotalPop&#39;]] . df.head() . State County TotalPop . 0 Alabama | Autauga County | 55036 | . 1 Alabama | Baldwin County | 203360 | . 2 Alabama | Barbour County | 26201 | . 3 Alabama | Bibb County | 22580 | . 4 Alabama | Blount County | 57667 | . Using the method from the blog post, I can make a grouped dataframe that sums up the populations for each county: . df.groupby(&#39;State&#39;).agg( state_pop = pd.NamedAgg(column=&#39;TotalPop&#39;, aggfunc=&#39;sum&#39;) ).head(7) . state_pop . State . Alabama 4850771 | . Alaska 738565 | . Arizona 6809946 | . Arkansas 2977944 | . California 38982847 | . Colorado 5436519 | . Connecticut 3594478 | . If I wrap it in a .join(), I can add it back to the original dataframe to use later: . df.join(df.groupby(&#39;State&#39;).agg( state_pop = pd.NamedAgg(column=&#39;TotalPop&#39;, aggfunc=&#39;sum&#39;) ), on=&#39;State&#39;).head(7) . State County TotalPop state_pop . 0 Alabama | Autauga County | 55036 | 4850771 | . 1 Alabama | Baldwin County | 203360 | 4850771 | . 2 Alabama | Barbour County | 26201 | 4850771 | . 3 Alabama | Bibb County | 22580 | 4850771 | . 4 Alabama | Blount County | 57667 | 4850771 | . 5 Alabama | Bullock County | 10478 | 4850771 | . 6 Alabama | Butler County | 20126 | 4850771 | . Overall . Grouping data happens a bunch, but it can be complicated to remember the mechanics. It can also feel sometimes like blog posts are shouts into a void, but they can be the best teaching tools out there! Gotta say thanks again to Shane Lynn; I review the screenshot at the top of your blog post often. . Image Credit . merged dataframes by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/python/pandas/2020/07/28/python-grouping-example.html",
            "relUrl": "/python/pandas/2020/07/28/python-grouping-example.html",
            "date": " • Jul 28, 2020"
        }
        
    
  
    
  
    
        ,"post5": {
            "title": "Twelve Clock List",
            "content": "Documenting the options for a clock game . . ...What? . Whenever I see a digital clock, I do a little math in my head. I try to take the numbers and, using only addition, subtraction, multiplication and division, get them to come out to twelve. I thought I&#39;d try to get an exhaustive list of times that this trick applies to. Let&#39;s figure it out! . import re import pandas as pd import seaborn as sns import matplotlib.pyplot as plt . Get with the times . First we need to get a list of the times. I&#39;m going to work with strings. Let&#39;s go through some numbers and rule out invalid ones by places. . valid = [] for x in range(100, 1260): num = &#39;&#39; num += str(x) if len(num) == 3: if int(num[0]) &lt;= 9 and int(num[1]) &lt;= 5: valid.append(num) if len(num) == 4: if int(num[0]) &lt;= 1 and int(num[1]) &lt;= 2 and int(num[2]) &lt;= 5: valid.append(num) . Cool. We can check that we have 720, which is half of all times since they all repeat once. Now we can go about making possible math expressions. . len(valid) . 720 . valid[:4] . [&#39;100&#39;, &#39;101&#39;, &#39;102&#39;, &#39;103&#39;] . Prep the times . To do this, I&#39;m going to put together strings of math expressions for all permutations of the numbers, then evaluate them and see which ones work. Certainly brute force, but it&#39;ll work. First, I make a dictionary of all permutations of the numbers for each time. . from itertools import permutations def allPermutations(str): perm_array = [] # Get all permutations of string &#39;ABC&#39; permList = permutations(str) # print all permutations for perm in list(permList): result = &#39;&#39;.join(perm) perm_array.append(result) return list(set(perm_array)) . time_dict = {} for time in valid: time_dict[time] = allPermutations(time) . time_dict[&#39;957&#39;] . [&#39;795&#39;, &#39;957&#39;, &#39;579&#39;, &#39;597&#39;, &#39;759&#39;, &#39;975&#39;] . Now, I go and add in all possible combinations of operators (+, -, *, /) in between the numbers. I also add in parens since eval follows PEMDAS and we want to avoid that in many cases. . equations = {} for time, perms in time_dict.items(): eq = &#39;&#39; ops = &#39;+-*/&#39; if len(time) == 3: equations[time] = [] for perm in perms: for a in ops: for b in ops: eq = f&quot;{perm[0]}{a}{perm[1]}{b}{perm[2]}&quot; eq = re.sub(r&#39;([0-9])&#39;, r&#39; 1)&#39;, eq) eq = &#39;(((&#39; + eq equations[time].append(eq) if len(time) == 4: equations[time] = [] for perm in perms: for a in ops: for b in ops: for c in ops: eq = f&quot;{perm[0]}{a}{perm[1]}{b}{perm[2]}{c}{perm[3]}&quot; eq = re.sub(r&#39;([0-9])&#39;, r&#39; 1)&#39;, eq) eq = &#39;((((&#39; + eq equations[time].append(eq) . equations[&#39;100&#39;][:10] . [&#39;(((0)+0)+1)&#39;, &#39;(((0)+0)-1)&#39;, &#39;(((0)+0)*1)&#39;, &#39;(((0)+0)/1)&#39;, &#39;(((0)-0)+1)&#39;, &#39;(((0)-0)-1)&#39;, &#39;(((0)-0)*1)&#39;, &#39;(((0)-0)/1)&#39;, &#39;(((0)*0)+1)&#39;, &#39;(((0)*0)-1)&#39;] . Look at the time . Cool. Now that we have a bunch of expressions, let&#39;s evaluate them and see what we get. . they_work = {} for time, eq_list in equations.items(): for eq in eq_list: try: result = eval(eq) except ZeroDivisionError: continue if result == 12 and time not in they_work.keys(): they_work[time] = eq . they_work . {&#39;116&#39;: &#39;(((1)+1)*6)&#39;, &#39;124&#39;: &#39;(((2)+1)*4)&#39;, &#39;125&#39;: &#39;(((5)+1)*2)&#39;, &#39;126&#39;: &#39;(((6)*1)*2)&#39;, &#39;127&#39;: &#39;(((7)-1)*2)&#39;, &#39;129&#39;: &#39;(((9)+1)+2)&#39;, &#39;133&#39;: &#39;(((1)+3)*3)&#39;, &#39;134&#39;: &#39;(((1)*4)*3)&#39;, &#39;135&#39;: &#39;(((5)-1)*3)&#39;, &#39;136&#39;: &#39;(((3)-1)*6)&#39;, &#39;138&#39;: &#39;(((1)+3)+8)&#39;, &#39;139&#39;: &#39;(((3)+9)*1)&#39;, &#39;142&#39;: &#39;(((2)+1)*4)&#39;, &#39;143&#39;: &#39;(((1)*4)*3)&#39;, &#39;144&#39;: &#39;(((4)-1)*4)&#39;, &#39;147&#39;: &#39;(((7)+4)+1)&#39;, &#39;148&#39;: &#39;(((8)+4)*1)&#39;, &#39;149&#39;: &#39;(((9)+4)-1)&#39;, &#39;152&#39;: &#39;(((5)+1)*2)&#39;, &#39;153&#39;: &#39;(((5)-1)*3)&#39;, &#39;156&#39;: &#39;(((6)+1)+5)&#39;, &#39;157&#39;: &#39;(((1)*7)+5)&#39;, &#39;158&#39;: &#39;(((5)-1)+8)&#39;, &#39;206&#39;: &#39;(((2)+0)*6)&#39;, &#39;214&#39;: &#39;(((2)+1)*4)&#39;, &#39;215&#39;: &#39;(((5)+1)*2)&#39;, &#39;216&#39;: &#39;(((6)*1)*2)&#39;, &#39;217&#39;: &#39;(((7)-1)*2)&#39;, &#39;219&#39;: &#39;(((9)+1)+2)&#39;, &#39;223&#39;: &#39;(((3)*2)*2)&#39;, &#39;224&#39;: &#39;(((2)+4)*2)&#39;, &#39;225&#39;: &#39;(((5)*2)+2)&#39;, &#39;227&#39;: &#39;(((7)*2)-2)&#39;, &#39;228&#39;: &#39;(((2)+2)+8)&#39;, &#39;232&#39;: &#39;(((3)*2)*2)&#39;, &#39;233&#39;: &#39;(((3)+3)*2)&#39;, &#39;236&#39;: &#39;(((2)*3)+6)&#39;, &#39;237&#39;: &#39;(((2)+3)+7)&#39;, &#39;238&#39;: &#39;(((3)/2)*8)&#39;, &#39;239&#39;: &#39;(((9)-3)*2)&#39;, &#39;241&#39;: &#39;(((2)+1)*4)&#39;, &#39;242&#39;: &#39;(((2)+4)*2)&#39;, &#39;244&#39;: &#39;(((4)*2)+4)&#39;, &#39;245&#39;: &#39;(((5)-2)*4)&#39;, &#39;246&#39;: &#39;(((4)+6)+2)&#39;, &#39;248&#39;: &#39;(((2)*8)-4)&#39;, &#39;251&#39;: &#39;(((5)+1)*2)&#39;, &#39;252&#39;: &#39;(((5)*2)+2)&#39;, &#39;254&#39;: &#39;(((5)-2)*4)&#39;, &#39;255&#39;: &#39;(((2)+5)+5)&#39;, &#39;259&#39;: &#39;(((5)-2)+9)&#39;, &#39;304&#39;: &#39;(((0)+4)*3)&#39;, &#39;309&#39;: &#39;(((0)+3)+9)&#39;, &#39;313&#39;: &#39;(((1)+3)*3)&#39;, &#39;314&#39;: &#39;(((1)*4)*3)&#39;, &#39;315&#39;: &#39;(((5)-1)*3)&#39;, &#39;316&#39;: &#39;(((3)-1)*6)&#39;, &#39;318&#39;: &#39;(((1)+3)+8)&#39;, &#39;319&#39;: &#39;(((3)+9)*1)&#39;, &#39;322&#39;: &#39;(((2)+2)*3)&#39;, &#39;323&#39;: &#39;(((3)+3)*2)&#39;, &#39;326&#39;: &#39;(((2)*3)+6)&#39;, &#39;327&#39;: &#39;(((2)+3)+7)&#39;, &#39;328&#39;: &#39;(((3)/2)*8)&#39;, &#39;329&#39;: &#39;(((9)-3)*2)&#39;, &#39;331&#39;: &#39;(((1)+3)*3)&#39;, &#39;332&#39;: &#39;(((3)+3)*2)&#39;, &#39;333&#39;: &#39;(((3)*3)+3)&#39;, &#39;335&#39;: &#39;(((5)*3)-3)&#39;, &#39;336&#39;: &#39;(((3)+6)+3)&#39;, &#39;337&#39;: &#39;(((7)-3)*3)&#39;, &#39;340&#39;: &#39;(((0)+4)*3)&#39;, &#39;341&#39;: &#39;(((1)*4)*3)&#39;, &#39;345&#39;: &#39;(((5)+3)+4)&#39;, &#39;346&#39;: &#39;(((6)-3)*4)&#39;, &#39;348&#39;: &#39;(((8)-4)*3)&#39;, &#39;349&#39;: &#39;(((9)/3)*4)&#39;, &#39;351&#39;: &#39;(((5)-1)*3)&#39;, &#39;353&#39;: &#39;(((5)*3)-3)&#39;, &#39;354&#39;: &#39;(((5)+3)+4)&#39;, &#39;356&#39;: &#39;(((5)-3)*6)&#39;, &#39;359&#39;: &#39;(((9)-5)*3)&#39;, &#39;403&#39;: &#39;(((0)+4)*3)&#39;, &#39;408&#39;: &#39;(((8)+4)+0)&#39;, &#39;412&#39;: &#39;(((2)+1)*4)&#39;, &#39;413&#39;: &#39;(((1)*4)*3)&#39;, &#39;414&#39;: &#39;(((4)-1)*4)&#39;, &#39;417&#39;: &#39;(((7)+4)+1)&#39;, &#39;418&#39;: &#39;(((8)+4)*1)&#39;, &#39;419&#39;: &#39;(((9)+4)-1)&#39;, &#39;421&#39;: &#39;(((2)+1)*4)&#39;, &#39;422&#39;: &#39;(((2)+4)*2)&#39;, &#39;424&#39;: &#39;(((4)*2)+4)&#39;, &#39;425&#39;: &#39;(((5)-2)*4)&#39;, &#39;426&#39;: &#39;(((2)+6)+4)&#39;, &#39;428&#39;: &#39;(((2)*8)-4)&#39;, &#39;430&#39;: &#39;(((0)+4)*3)&#39;, &#39;431&#39;: &#39;(((1)*4)*3)&#39;, &#39;435&#39;: &#39;(((5)+3)+4)&#39;, &#39;436&#39;: &#39;(((6)-3)*4)&#39;, &#39;438&#39;: &#39;(((8)-4)*3)&#39;, &#39;439&#39;: &#39;(((9)/3)*4)&#39;, &#39;441&#39;: &#39;(((4)-1)*4)&#39;, &#39;442&#39;: &#39;(((2)*4)+4)&#39;, &#39;444&#39;: &#39;(((4)+4)+4)&#39;, &#39;447&#39;: &#39;(((7)-4)*4)&#39;, &#39;452&#39;: &#39;(((5)-2)*4)&#39;, &#39;453&#39;: &#39;(((5)+3)+4)&#39;, &#39;458&#39;: &#39;(((8)-5)*4)&#39;, &#39;507&#39;: &#39;(((5)+7)+0)&#39;, &#39;512&#39;: &#39;(((5)+1)*2)&#39;, &#39;513&#39;: &#39;(((5)-1)*3)&#39;, &#39;516&#39;: &#39;(((5)+1)+6)&#39;, &#39;517&#39;: &#39;(((1)*7)+5)&#39;, &#39;518&#39;: &#39;(((5)-1)+8)&#39;, &#39;521&#39;: &#39;(((5)+1)*2)&#39;, &#39;522&#39;: &#39;(((5)*2)+2)&#39;, &#39;524&#39;: &#39;(((5)-2)*4)&#39;, &#39;525&#39;: &#39;(((2)+5)+5)&#39;, &#39;529&#39;: &#39;(((5)-2)+9)&#39;, &#39;531&#39;: &#39;(((5)-1)*3)&#39;, &#39;533&#39;: &#39;(((5)*3)-3)&#39;, &#39;534&#39;: &#39;(((5)+3)+4)&#39;, &#39;536&#39;: &#39;(((5)-3)*6)&#39;, &#39;539&#39;: &#39;(((9)-5)*3)&#39;, &#39;542&#39;: &#39;(((5)-2)*4)&#39;, &#39;543&#39;: &#39;(((5)+3)+4)&#39;, &#39;548&#39;: &#39;(((8)-5)*4)&#39;, &#39;552&#39;: &#39;(((2)+5)+5)&#39;, &#39;602&#39;: &#39;(((2)+0)*6)&#39;, &#39;606&#39;: &#39;(((6)+6)+0)&#39;, &#39;611&#39;: &#39;(((1)+1)*6)&#39;, &#39;612&#39;: &#39;(((6)*1)*2)&#39;, &#39;613&#39;: &#39;(((3)-1)*6)&#39;, &#39;615&#39;: &#39;(((6)+1)+5)&#39;, &#39;616&#39;: &#39;(((1)*6)+6)&#39;, &#39;617&#39;: &#39;(((6)+7)-1)&#39;, &#39;620&#39;: &#39;(((2)+0)*6)&#39;, &#39;621&#39;: &#39;(((6)*1)*2)&#39;, &#39;623&#39;: &#39;(((2)*3)+6)&#39;, &#39;624&#39;: &#39;(((4)+6)+2)&#39;, &#39;628&#39;: &#39;(((6)+8)-2)&#39;, &#39;629&#39;: &#39;(((9)*2)-6)&#39;, &#39;631&#39;: &#39;(((3)-1)*6)&#39;, &#39;632&#39;: &#39;(((2)*3)+6)&#39;, &#39;633&#39;: &#39;(((3)+6)+3)&#39;, &#39;634&#39;: &#39;(((6)-3)*4)&#39;, &#39;635&#39;: &#39;(((5)-3)*6)&#39;, &#39;636&#39;: &#39;(((3)*6)-6)&#39;, &#39;639&#39;: &#39;(((9)-3)+6)&#39;, &#39;642&#39;: &#39;(((2)+6)+4)&#39;, &#39;643&#39;: &#39;(((6)-3)*4)&#39;, &#39;646&#39;: &#39;(((6)-4)*6)&#39;, &#39;648&#39;: &#39;(((6)*8)/4)&#39;, &#39;649&#39;: &#39;(((9)-6)*4)&#39;, &#39;651&#39;: &#39;(((6)+1)+5)&#39;, &#39;653&#39;: &#39;(((5)-3)*6)&#39;, &#39;657&#39;: &#39;(((7)-5)*6)&#39;, &#39;705&#39;: &#39;(((0)+5)+7)&#39;, &#39;712&#39;: &#39;(((7)-1)*2)&#39;, &#39;714&#39;: &#39;(((7)+4)+1)&#39;, &#39;715&#39;: &#39;(((1)*7)+5)&#39;, &#39;716&#39;: &#39;(((6)+7)-1)&#39;, &#39;721&#39;: &#39;(((7)-1)*2)&#39;, &#39;722&#39;: &#39;(((2)*7)-2)&#39;, &#39;723&#39;: &#39;(((2)+3)+7)&#39;, &#39;727&#39;: &#39;(((7)+7)-2)&#39;, &#39;732&#39;: &#39;(((2)+3)+7)&#39;, &#39;733&#39;: &#39;(((7)-3)*3)&#39;, &#39;738&#39;: &#39;(((7)-3)+8)&#39;, &#39;739&#39;: &#39;(((7)*3)-9)&#39;, &#39;741&#39;: &#39;(((7)+4)+1)&#39;, &#39;744&#39;: &#39;(((7)-4)*4)&#39;, &#39;749&#39;: &#39;(((9)-4)+7)&#39;, &#39;750&#39;: &#39;(((0)+5)+7)&#39;, &#39;751&#39;: &#39;(((1)*7)+5)&#39;, &#39;756&#39;: &#39;(((7)-5)*6)&#39;, &#39;804&#39;: &#39;(((8)+4)+0)&#39;, &#39;813&#39;: &#39;(((1)+3)+8)&#39;, &#39;814&#39;: &#39;(((8)+4)*1)&#39;, &#39;815&#39;: &#39;(((5)-1)+8)&#39;, &#39;822&#39;: &#39;(((8)+2)+2)&#39;, &#39;823&#39;: &#39;(((3)/2)*8)&#39;, &#39;824&#39;: &#39;(((2)*8)-4)&#39;, &#39;826&#39;: &#39;(((6)+8)-2)&#39;, &#39;828&#39;: &#39;(((8)/2)+8)&#39;, &#39;831&#39;: &#39;(((1)+3)+8)&#39;, &#39;832&#39;: &#39;(((3)/2)*8)&#39;, &#39;834&#39;: &#39;(((8)-4)*3)&#39;, &#39;837&#39;: &#39;(((7)-3)+8)&#39;, &#39;840&#39;: &#39;(((8)+4)+0)&#39;, &#39;841&#39;: &#39;(((8)+4)*1)&#39;, &#39;842&#39;: &#39;(((2)*8)-4)&#39;, &#39;843&#39;: &#39;(((8)-4)*3)&#39;, &#39;845&#39;: &#39;(((8)-5)*4)&#39;, &#39;846&#39;: &#39;(((6)*8)/4)&#39;, &#39;848&#39;: &#39;(((8)-4)+8)&#39;, &#39;851&#39;: &#39;(((5)-1)+8)&#39;, &#39;854&#39;: &#39;(((8)-5)*4)&#39;, &#39;859&#39;: &#39;(((9)+8)-5)&#39;, &#39;903&#39;: &#39;(((0)+3)+9)&#39;, &#39;912&#39;: &#39;(((9)+1)+2)&#39;, &#39;913&#39;: &#39;(((3)+9)*1)&#39;, &#39;914&#39;: &#39;(((9)+4)-1)&#39;, &#39;921&#39;: &#39;(((9)+1)+2)&#39;, &#39;923&#39;: &#39;(((9)-3)*2)&#39;, &#39;925&#39;: &#39;(((5)-2)+9)&#39;, &#39;926&#39;: &#39;(((9)*2)-6)&#39;, &#39;930&#39;: &#39;(((0)+3)+9)&#39;, &#39;931&#39;: &#39;(((3)+9)*1)&#39;, &#39;932&#39;: &#39;(((9)-3)*2)&#39;, &#39;934&#39;: &#39;(((9)/3)*4)&#39;, &#39;935&#39;: &#39;(((9)-5)*3)&#39;, &#39;936&#39;: &#39;(((9)-3)+6)&#39;, &#39;937&#39;: &#39;(((7)*3)-9)&#39;, &#39;939&#39;: &#39;(((9)/3)+9)&#39;, &#39;941&#39;: &#39;(((9)+4)-1)&#39;, &#39;943&#39;: &#39;(((9)/3)*4)&#39;, &#39;946&#39;: &#39;(((9)-6)*4)&#39;, &#39;947&#39;: &#39;(((9)-4)+7)&#39;, &#39;952&#39;: &#39;(((5)-2)+9)&#39;, &#39;953&#39;: &#39;(((9)-5)*3)&#39;, &#39;958&#39;: &#39;(((9)+8)-5)&#39;, &#39;1016&#39;: &#39;((((1)+1)+0)*6)&#39;, &#39;1024&#39;: &#39;((((1)+2)+0)*4)&#39;, &#39;1025&#39;: &#39;((((5)+0)+1)*2)&#39;, &#39;1026&#39;: &#39;((((1)*2)*6)+0)&#39;, &#39;1027&#39;: &#39;((((7)+0)-1)*2)&#39;, &#39;1029&#39;: &#39;((((1)+9)+0)+2)&#39;, &#39;1033&#39;: &#39;((((3)+1)+0)*3)&#39;, &#39;1034&#39;: &#39;((((1)*3)*4)+0)&#39;, &#39;1035&#39;: &#39;((((5)+0)-1)*3)&#39;, &#39;1036&#39;: &#39;((((3)-1)+0)*6)&#39;, &#39;1038&#39;: &#39;((((1)+8)+0)+3)&#39;, &#39;1039&#39;: &#39;((((0)+9)*1)+3)&#39;, &#39;1042&#39;: &#39;((((1)+2)+0)*4)&#39;, &#39;1043&#39;: &#39;((((1)*3)*4)+0)&#39;, &#39;1044&#39;: &#39;((((4)-1)+0)*4)&#39;, &#39;1047&#39;: &#39;((((0)+1)+4)+7)&#39;, &#39;1048&#39;: &#39;((((8)*1)+4)+0)&#39;, &#39;1049&#39;: &#39;((((4)+0)+9)-1)&#39;, &#39;1052&#39;: &#39;((((5)+0)+1)*2)&#39;, &#39;1053&#39;: &#39;((((5)+0)-1)*3)&#39;, &#39;1056&#39;: &#39;((((6)+0)+5)+1)&#39;, &#39;1057&#39;: &#39;((((7)+0)+5)*1)&#39;, &#39;1058&#39;: &#39;((((8)+0)+5)-1)&#39;, &#39;1106&#39;: &#39;((((1)+1)+0)*6)&#39;, &#39;1114&#39;: &#39;((((1)+1)+1)*4)&#39;, &#39;1116&#39;: &#39;((((1)+1)*6)*1)&#39;, &#39;1119&#39;: &#39;((((9)+1)+1)+1)&#39;, &#39;1123&#39;: &#39;((((2)+1)+1)*3)&#39;, &#39;1124&#39;: &#39;((((4)+1)+1)*2)&#39;, &#39;1125&#39;: &#39;((((2)*5)+1)+1)&#39;, &#39;1126&#39;: &#39;((((2)*1)*6)*1)&#39;, &#39;1127&#39;: &#39;((((7)-1)*2)*1)&#39;, &#39;1128&#39;: &#39;((((2)+1)+1)+8)&#39;, &#39;1129&#39;: &#39;((((1)+2)*1)+9)&#39;, &#39;1132&#39;: &#39;((((2)+1)+1)*3)&#39;, &#39;1133&#39;: &#39;((((1)*1)+3)*3)&#39;, &#39;1134&#39;: &#39;((((3)*1)*4)*1)&#39;, &#39;1135&#39;: &#39;((((5)-1)*3)*1)&#39;, &#39;1136&#39;: &#39;((((1)*3)-1)*6)&#39;, &#39;1137&#39;: &#39;((((3)+1)+1)+7)&#39;, &#39;1138&#39;: &#39;((((3)+1)+8)*1)&#39;, &#39;1139&#39;: &#39;((((9)+3)+1)-1)&#39;, &#39;1141&#39;: &#39;((((1)+1)+1)*4)&#39;, &#39;1142&#39;: &#39;((((4)+1)+1)*2)&#39;, &#39;1143&#39;: &#39;((((3)*1)*4)*1)&#39;, &#39;1144&#39;: &#39;((((4)-1)*4)*1)&#39;, &#39;1145&#39;: &#39;((((5)-1)-1)*4)&#39;, &#39;1146&#39;: &#39;((((1)+4)+1)+6)&#39;, &#39;1147&#39;: &#39;((((1)+7)+4)*1)&#39;, &#39;1148&#39;: &#39;((((1)+8)+4)-1)&#39;, &#39;1149&#39;: &#39;((((4)+9)-1)*1)&#39;, &#39;1152&#39;: &#39;((((2)*5)+1)+1)&#39;, &#39;1153&#39;: &#39;((((5)-1)*3)*1)&#39;, &#39;1154&#39;: &#39;((((5)-1)-1)*4)&#39;, &#39;1155&#39;: &#39;((((5)+1)+5)+1)&#39;, &#39;1156&#39;: &#39;((((1)+6)+5)*1)&#39;, &#39;1157&#39;: &#39;((((7)+1)+5)-1)&#39;, &#39;1158&#39;: &#39;((((5)-1)+8)*1)&#39;, &#39;1159&#39;: &#39;((((5)-1)+9)-1)&#39;, &#39;1204&#39;: &#39;((((1)+2)+0)*4)&#39;, &#39;1205&#39;: &#39;((((5)+0)+1)*2)&#39;, &#39;1206&#39;: &#39;((((1)*2)*6)+0)&#39;, &#39;1207&#39;: &#39;((((7)+0)-1)*2)&#39;, &#39;1209&#39;: &#39;((((1)+9)+0)+2)&#39;, &#39;1213&#39;: &#39;((((2)+1)+1)*3)&#39;, &#39;1214&#39;: &#39;((((4)+1)+1)*2)&#39;, &#39;1215&#39;: &#39;((((2)*5)+1)+1)&#39;, &#39;1216&#39;: &#39;((((2)*1)*6)*1)&#39;, &#39;1217&#39;: &#39;((((7)-1)*2)*1)&#39;, &#39;1218&#39;: &#39;((((2)+1)+1)+8)&#39;, &#39;1219&#39;: &#39;((((1)+2)*1)+9)&#39;, &#39;1222&#39;: &#39;((((1)+2)*2)*2)&#39;, &#39;1223&#39;: &#39;((((2)+2)*3)*1)&#39;, &#39;1224&#39;: &#39;((((1)+4)*2)+2)&#39;, &#39;1225&#39;: &#39;((((2)+5)-1)*2)&#39;, &#39;1226&#39;: &#39;((((2)/2)+1)*6)&#39;, &#39;1227&#39;: &#39;((((1)+2)+7)+2)&#39;, &#39;1228&#39;: &#39;((((2)+1)*8)/2)&#39;, &#39;1229&#39;: &#39;((((2)+9)-1)+2)&#39;, &#39;1231&#39;: &#39;((((1)+1)*3)*2)&#39;, &#39;1232&#39;: &#39;((((2)+2)*3)*1)&#39;, &#39;1233&#39;: &#39;((((3)-1)*3)*2)&#39;, &#39;1234&#39;: &#39;((((3)+1)*2)+4)&#39;, &#39;1235&#39;: &#39;((((2)*5)+3)-1)&#39;, &#39;1236&#39;: &#39;((((6)+3)+2)+1)&#39;, &#39;1237&#39;: &#39;((((1)*3)+2)+7)&#39;, &#39;1238&#39;: &#39;((((2)+3)-1)+8)&#39;, &#39;1239&#39;: &#39;((((9)-1)*3)/2)&#39;, &#39;1240&#39;: &#39;((((1)+2)+0)*4)&#39;, &#39;1241&#39;: &#39;((((4)+1)+1)*2)&#39;, &#39;1242&#39;: &#39;((((1)+4)*2)+2)&#39;, &#39;1243&#39;: &#39;((((3)+1)*2)+4)&#39;, &#39;1244&#39;: &#39;((((2)*4)+4)*1)&#39;, &#39;1245&#39;: &#39;((((5)+1)+4)+2)&#39;, &#39;1246&#39;: &#39;((((1)*6)+2)+4)&#39;, &#39;1247&#39;: &#39;((((4)+2)+7)-1)&#39;, &#39;1248&#39;: &#39;((((8)*1)*2)-4)&#39;, &#39;1249&#39;: &#39;((((4)-2)+9)+1)&#39;, &#39;1250&#39;: &#39;((((5)+0)+1)*2)&#39;, &#39;1251&#39;: &#39;((((2)*5)+1)+1)&#39;, &#39;1252&#39;: &#39;((((2)+5)-1)*2)&#39;, &#39;1253&#39;: &#39;((((2)*5)+3)-1)&#39;, &#39;1254&#39;: &#39;((((5)+1)+4)+2)&#39;, &#39;1255&#39;: &#39;((((1)*5)+2)+5)&#39;, &#39;1256&#39;: &#39;((((5)+2)+6)-1)&#39;, &#39;1257&#39;: &#39;((((2)-1)*7)+5)&#39;, &#39;1258&#39;: &#39;((((8)+1)-2)+5)&#39;, &#39;1259&#39;: &#39;((((2)*9)-5)-1)&#39;} . print(f&quot;So this trick works {len(they_work.keys()) / len(valid) * 100:.2f}% of the time&quot;) . So this trick works 45.97% of the time . Well...there ya go . There are a bunch that are easy to spot. Was interesting to see ones where division is used. . [x for x in they_work.values() if &#39;/&#39; in x] . [&#39;(((3)/2)*8)&#39;, &#39;(((3)/2)*8)&#39;, &#39;(((9)/3)*4)&#39;, &#39;(((9)/3)*4)&#39;, &#39;(((6)*8)/4)&#39;, &#39;(((3)/2)*8)&#39;, &#39;(((8)/2)+8)&#39;, &#39;(((3)/2)*8)&#39;, &#39;(((6)*8)/4)&#39;, &#39;(((9)/3)*4)&#39;, &#39;(((9)/3)+9)&#39;, &#39;(((9)/3)*4)&#39;, &#39;((((2)/2)+1)*6)&#39;, &#39;((((2)+1)*8)/2)&#39;, &#39;((((9)-1)*3)/2)&#39;] . Maybe you&#39;ll give it a try next time you spot a digital clock... . DataFrame of the results . df = pd.DataFrame(valid) df.rename(columns={0: &#39;raw&#39;}, inplace=True) . df[&#39;raw_temp&#39;] = df[&#39;raw&#39;].apply(lambda x: &#39;0&#39; + x if len(x) == 3 else x) . df[&#39;time&#39;] = df[&#39;raw_temp&#39;].apply(lambda x: pd.to_datetime(x, format=&#39;%H%M&#39;)) . df.head() . raw raw_temp time twelve . 0 100 | 0100 | 1900-01-01 01:00:00 | False | . 1 101 | 0101 | 1900-01-01 01:01:00 | False | . 2 102 | 0102 | 1900-01-01 01:02:00 | False | . 3 103 | 0103 | 1900-01-01 01:03:00 | False | . 4 104 | 0104 | 1900-01-01 01:04:00 | False | . valid_times = list(they_work.keys()) . df[&#39;twelve&#39;] = df[&#39;raw&#39;].isin(valid_times) . df.head() . raw raw_temp time twelve . 0 100 | 0100 | 1900-01-01 01:00:00 | False | . 1 101 | 0101 | 1900-01-01 01:01:00 | False | . 2 102 | 0102 | 1900-01-01 01:02:00 | False | . 3 103 | 0103 | 1900-01-01 01:03:00 | False | . 4 104 | 0104 | 1900-01-01 01:04:00 | False | . Image Credit . Twelve by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/python/pandas/2020/06/04/digital-twelve.html",
            "relUrl": "/python/pandas/2020/06/04/digital-twelve.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Looking into YouTube Free Movies",
            "content": ". I saw that YouTube had a wide selection of movies, but the free movies didn&#39;t seem very good. Was interested in seeing what kind of ratings the free movies on YouTube receive. Let&#39;s figure it out! . Go to the Movies . To start, let&#39;s import some things. . import pandas as pd import requests from bs4 import BeautifulSoup import seaborn as sns import matplotlib.pyplot as plt . We can use requests to grab the html. . url = &#39;https://www.youtube.com/feed/storefront?bp=kgEmCGQSIlBMSFBUeFR4dEMwaWJWWnJUMl9XS1dVbDJTQXhzS3VLd3iiBQIoAg%3D%3D&#39; page = requests.get(url) . soup = BeautifulSoup(page.text, &#39;html.parser&#39;) . print(soup.prettify()[:200]) . &lt;!DOCTYPE html&gt; &lt;html data-cast-api-enabled=&#34;true&#34; lang=&#34;en&#34;&gt; &lt;head&gt; &lt;style name=&#34;www-roboto&#34;&gt; @font-face{font-family:&#39;Roboto&#39;;font-style:normal;font-weight:500;src:local(&#39;Roboto Medium&#39;),local( . At this point, we can do some inspecting to find that each film has a yt-lockup-title class that we can access. With a little bit of fiddling, we can get a dataframe of the movie names. There is also a common format which we can use to split the strings into separate columns. . html_films = soup.find_all(class_=&quot;yt-lockup-title&quot;) for film in html_films[:5]: print(film.get_text()) . Dino King - Duration: 1:28:47. Snow Queen - Duration: 1:16:07. Beyond Beyond - Duration: 1:19:24. Igor (U.S) - Duration: 1:26:31. Sleepover - Duration: 1:29:29. . movies = [film.get_text() for film in html_films] . movies[:6] . [&#39;Dino King - Duration: 1:28:47.&#39;, &#39;Snow Queen - Duration: 1:16:07.&#39;, &#39;Beyond Beyond - Duration: 1:19:24.&#39;, &#39;Igor (U.S) - Duration: 1:26:31.&#39;, &#39;Sleepover - Duration: 1:29:29.&#39;, &#39;The Secret of Nimh - Duration: 1:22:46.&#39;] . df = pd.DataFrame(movies) df.rename(columns={0: &#39;movie&#39;}, inplace=True) . df[df.movie.str.contains(&#39; - Duration: &#39;)].head() . movie . 0 Dino King - Duration: 1:28:47. | . 1 Snow Queen - Duration: 1:16:07. | . 2 Beyond Beyond - Duration: 1:19:24. | . 3 Igor (U.S) - Duration: 1:26:31. | . 4 Sleepover - Duration: 1:29:29. | . df = df.movie.str.split(&#39; - Duration: &#39;, expand=True) . df[1] = df[1].str.rstrip(&#39;.&#39;) . df = df.reset_index() . df.rename(columns={0: &#39;yt_title&#39;, 1: &#39;yt_duration&#39;, &#39;index&#39;: &#39;yt_id&#39;}, inplace=True) . df.head() . yt_id yt_title yt_duration . 0 0 | Dino King | 1:28:47 | . 1 1 | Snow Queen | 1:16:07 | . 2 2 | Beyond Beyond | 1:19:24 | . 3 3 | Igor (U.S) | 1:26:31 | . 4 4 | Sleepover | 1:29:29 | . Convert Duration to minutes . The string movie time doesn&#39;t mean much to us. Let&#39;s turn it into minutes. . def split_time(x): numbers = x.split(&#39;:&#39;) time = int(numbers[0]) * 60 + int(numbers[1]) return time . df[&#39;yt_minutes&#39;] = df[&#39;yt_duration&#39;].apply(split_time) . IMDb Data . We will use data from the IMDb datasets to get info on the movie ratings. | . imdb_ratings = pd.read_csv(&#39;/Users/zachbogart/Downloads/title.ratings.tsv&#39;, sep=&#39; t&#39;) imdb_basics = pd.read_csv(&#39;/Users/zachbogart/Downloads/title.basics.tsv&#39;, sep=&#39; t&#39;) . /Users/zachbogart/Documents/venv-breadbasket/sourdough_venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . imdb = imdb_basics.merge(imdb_ratings, how=&#39;left&#39;, on=&#39;tconst&#39;) . imdb.shape . (6831547, 11) . Let&#39;s look just at items that are movies . imdb.titleType.value_counts() . tvEpisode 4869408 short 741081 movie 551301 video 265727 tvSeries 184466 tvMovie 121175 tvMiniSeries 31078 tvSpecial 29209 videoGame 25548 tvShort 12554 Name: titleType, dtype: int64 . imdb = imdb.loc[imdb.titleType == &#39;movie&#39;] . imdb.shape . (551301, 11) . imdb.head() . tconst titleType primaryTitle originalTitle isAdult startYear endYear runtimeMinutes genres averageRating numVotes . 8 tt0000009 | movie | Miss Jerry | Miss Jerry | 0 | 1894 | N | 45 | Romance | 5.9 | 153.0 | . 145 tt0000147 | movie | The Corbett-Fitzsimmons Fight | The Corbett-Fitzsimmons Fight | 0 | 1897 | N | 20 | Documentary,News,Sport | 5.2 | 346.0 | . 332 tt0000335 | movie | Soldiers of the Cross | Soldiers of the Cross | 0 | 1900 | N | N | Biography,Drama | 6.1 | 40.0 | . 499 tt0000502 | movie | Bohemios | Bohemios | 0 | 1905 | N | 100 | N | 3.8 | 6.0 | . 571 tt0000574 | movie | The Story of the Kelly Gang | The Story of the Kelly Gang | 0 | 1906 | N | 70 | Biography,Crime,Drama | 6.1 | 574.0 | . Try Joining . We are playing this pretty fast and loose, but we can get a rough sense of the movie ratings using a simple join and dropping any corner cases. . joined = df.merge(imdb, how=&#39;left&#39;, left_on=&#39;yt_title&#39;, right_on=&#39;primaryTitle&#39;) . joined = joined.dropna().sort_values(&#39;primaryTitle&#39;) . Let&#39;s not deal with any overlap. Just the movies that have one match (Could also try to match durations to get more movies, but let&#39;s ignore that right now). . there are a bunch of items with multiple results | Let&#39;s see what we have without trying to dig out overlapping movies | . joined.yt_title.value_counts() . Romeo and Juliet 10 Happily Ever After 10 Zoo 10 The Suspect 8 Gone 8 .. A Cowgirl&#39;s Story 1 Mad Money 1 Bakery in Brooklyn 1 Snow Queen 1 17 Miracles 1 Name: yt_title, Length: 262, dtype: int64 . singles = joined.groupby(&#39;yt_title&#39;)[&#39;yt_title&#39;].filter(lambda x: len(x) == 1) . easy = joined[joined.yt_title.isin(singles)] . What Do We Find? . The Highest (and Lowest) Rated . What are the top-rated movies available? Worst Rated? | . cols = [&#39;yt_title&#39;, &#39;averageRating&#39;, &#39;numVotes&#39;] . easy[cols].sort_values(&#39;averageRating&#39;, ascending=False).head(10) . yt_title averageRating numVotes . 478 The Usual Suspects | 8.5 | 956313.0 | . 429 Zeitgeist: Moving Forward | 8.2 | 17246.0 | . 440 Bones Brigade: An Autobiography | 8.1 | 2306.0 | . 433 Requiem for the American Dream | 8.1 | 8297.0 | . 712 Citizenfour | 8.0 | 50670.0 | . 413 Sound City | 7.8 | 11887.0 | . 308 Muscle Shoals | 7.7 | 3400.0 | . 303 Fat, Sick &amp; Nearly Dead | 7.5 | 8338.0 | . 272 The Secret of Roan Inish | 7.5 | 6792.0 | . 319 Unbranded | 7.4 | 1326.0 | . easy[cols].sort_values(&#39;averageRating&#39;).head(10) . yt_title averageRating numVotes . 10 Disaster Movie | 1.9 | 85558.0 | . 636 Alcatraz | 3.3 | 359.0 | . 169 Spiders 3D | 3.3 | 3960.0 | . 707 The Remains | 3.7 | 2870.0 | . 513 Fist of the Warrior | 3.8 | 369.0 | . 740 Fishing Naked | 4.0 | 782.0 | . 160 Ribbit | 4.0 | 611.0 | . 74 Hollow Creek | 4.2 | 592.0 | . 327 Arthur &amp; Merlin | 4.3 | 1226.0 | . 611 Out of Liberty | 4.4 | 262.0 | . How Old Are These Movies? . Most are a few years old. Some are decades old. | . easy[&#39;startYear&#39;] = pd.to_numeric(easy.startYear) . /Users/zachbogart/Documents/venv-breadbasket/sourdough_venv/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . ax = sns.relplot(data=easy, x=&#39;startYear&#39;, y=&#39;averageRating&#39;, kind=&#39;scatter&#39;, hue=&#39;yt_minutes&#39;) plt.show() . Scraping the Bottom of the Barrel . Looks like the movies are pretty poorly rated, averaging around 6 out of 10 | . ax = easy.averageRating.hist() plt.show() . print(f&quot;{easy.averageRating.mean():.2f}&quot;) . 5.95 . Overall . You get what you (don&#39;t) pay for. . Resources . https://docs.python-guide.org/scenarios/scrape/ | https://www.digitalocean.com/community/tutorials/how-to-work-with-web-data-using-requests-and-beautiful-soup-with-python-3 | https://www.geeksforgeeks.org/split-a-text-column-into-two-columns-in-pandas-dataframe/ | . Image Credit . integrated system by Zach Bogart from the Noun Project .",
            "url": "https://zachbogart.github.io/data-silience/python/pandas/beautifulsoup/seaborn/matplotlib/2020/05/25/youtube-movies.html",
            "relUrl": "/python/pandas/beautifulsoup/seaborn/matplotlib/2020/05/25/youtube-movies.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". What is this? . This is a place where I code up things. Maybe you’ll learn something. I know I will. . Who are you? . I’m Zach Bogart. Hi there! 👋 . You misspelled “Science” . I’ve probably misspelled other things, but that was intentional. . silience noun: the brilliant artistry hidden all around you.1 . How is this site made? . In case you’re interested, this website is powered by fastpages: . This site is built with fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. . – fastpages documentation . So glad to have come across this. 🙏 💖 . Can I give you a coffee? . Sure! I’m more of a tea or hot chocolate kind of guy, but sure! Much appreciated! . Anything else? . That’s about it. Darn! That’s the end! . . Here’s the full definition of silience from the Dictionary of Obscure Sorrows &#8617; . |",
          "url": "https://zachbogart.github.io/data-silience/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zachbogart.github.io/data-silience/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}